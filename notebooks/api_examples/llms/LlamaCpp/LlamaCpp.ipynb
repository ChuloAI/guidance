{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:43.388084400Z",
     "start_time": "2023-05-24T20:46:40.833523300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in c:\\deeplearningdev\\ai_experiments\\venv\\lib\\site-packages (0.1.52)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\deeplearningdev\\ai_experiments\\venv\\lib\\site-packages (from llama-cpp-python) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Install llama-cpp-python.\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import guidance.\n",
    "import guidance"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Create a LlamaCppSettings instance and set the model path and tokenizer name.\n",
    "settings = guidance.llms.LlamaCppSettings()\n",
    "settings.model = \"../ggml-v2-models/WizardLM-7B-uncensored.ggml.q5_1.bin\"\n",
    "# Hugginface original tokenizer needed at the moment.\n",
    "settings.tokenizer_name = \"ehartford/WizardLM-7B-Uncensored\"\n",
    "\n",
    "# Other models would look like this:\n",
    "# settings.model = \"../ggml-v3-models/gpt4-x-vicuna-13B.ggmlv3.q5_1.bin\"\n",
    "# settings.tokenizer_name = \"TheBloke/gpt4-x-vicuna-13B-HF\"\n",
    "\n",
    "# Or like this:\n",
    "# settings.model = \"../ggml-v3-models/Manticore-13B.ggmlv3.q5_1.bin\"\n",
    "# settings.tokenizer_name = \"openaccess-ai-collective/manticore-13b\"\n",
    "\n",
    "# Set additional arguments like number of threads and gpu layers.\n",
    "settings.n_gpu_layers = 10\n",
    "settings.n_threads = 12"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:49.811939Z",
     "start_time": "2023-05-24T20:46:49.804877100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model path does not exist: ../ggml-v2-models/WizardLM-7B-uncensored.ggml.q5_1.bin",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Create a LlamaCpp instance and pass the settings to it.\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m llama \u001B[38;5;241m=\u001B[39m \u001B[43mguidance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLlamaCpp\u001B[49m\u001B[43m(\u001B[49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\DeepLearningDev\\AI_Experiments\\guidance_fork\\guidance\\guidance\\llms\\_llama_cpp.py:46\u001B[0m, in \u001B[0;36mLlamaCpp.__init__\u001B[1;34m(self, settings, caching, token_healing, acceleration, temperature, role_start, role_end)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msettings \u001B[38;5;241m=\u001B[39m settings\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_obj \u001B[38;5;241m=\u001B[39m \u001B[43mLlama\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_gpu_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_gpu_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf16_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf16_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_mlock\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_mlock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_ctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_ctx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlast_n_tokens_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlast_n_tokens_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogits_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits_all\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_mmap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_mmap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_size \u001B[38;5;241m=\u001B[39m llama_n_vocab(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_obj\u001B[38;5;241m.\u001B[39mctx)\n",
      "File \u001B[1;32mC:\\DeepLearningDev\\AI_Experiments\\venv\\Lib\\site-packages\\llama_cpp\\llama.py:153\u001B[0m, in \u001B[0;36mLlama.__init__\u001B[1;34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, verbose)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlora_path \u001B[38;5;241m=\u001B[39m lora_path\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(model_path):\n\u001B[1;32m--> 153\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel path does not exist: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx \u001B[38;5;241m=\u001B[39m llama_cpp\u001B[38;5;241m.\u001B[39mllama_init_from_file(\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\n\u001B[0;32m    157\u001B[0m )\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Model path does not exist: ../ggml-v2-models/WizardLM-7B-uncensored.ggml.q5_1.bin"
     ]
    }
   ],
   "source": [
    "# Create a LlamaCpp instance and pass the settings to it.\n",
    "llama = guidance.llms.LlamaCpp(settings=settings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:56.433152200Z",
     "start_time": "2023-05-24T20:46:55.570187500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 24\u001B[0m\n\u001B[0;32m      5\u001B[0m character_maker \u001B[38;5;241m=\u001B[39m guidance(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mThe following is a character profile for an RPG game in JSON format.\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124m```json\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124m{\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitems\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m: [\u001B[39m\u001B[38;5;124m{{\u001B[39m\u001B[38;5;124m#geneach \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitems\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m num_iterations=5 join=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m}}\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{{\u001B[39m\u001B[38;5;124mgen \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthis\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m temperature=0.7}}\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{{\u001B[39m\u001B[38;5;124m/geneach}}]\u001B[39m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124m}```\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# generate a character\u001B[39;00m\n\u001B[0;32m     21\u001B[0m char \u001B[38;5;241m=\u001B[39m character_maker(\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me1f491f7-7ab8-4dac-8c20-c92b5e7d883d\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     23\u001B[0m     description\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA quick and nimble fighter.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m---> 24\u001B[0m     valid_weapons\u001B[38;5;241m=\u001B[39mvalid_weapons, llm\u001B[38;5;241m=\u001B[39m\u001B[43mllama\u001B[49m\n\u001B[0;32m     25\u001B[0m )\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(char)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'llama' is not defined"
     ]
    }
   ],
   "source": [
    "# we can pre-define valid option sets\n",
    "valid_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\n",
    "\n",
    "# define the prompt\n",
    "character_maker = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\n",
    "```json\n",
    "{\n",
    "    \"id\": \"{{id}}\",\n",
    "    \"description\": \"{{description}}\",\n",
    "    \"name\": \"{{gen 'name' temperature=0.7}}\",\n",
    "    \"age\": {{gen 'age' pattern='[0-9]+' max_tokens=2 stop=','}},\n",
    "    \"armor\": \"{{#select 'armor'}}leather{{or}}chainmail{{or}}plate{{/select}}\",\n",
    "    \"weapon\": \"{{select 'weapon' options=valid_weapons}}\",\n",
    "    \"class\": \"{{gen 'class'}}\",\n",
    "    \"mantra\": \"{{gen 'mantra' temperature=0.7}}\",\n",
    "    \"strength\": {{gen 'strength' pattern='[0-9]+' max_tokens=2 stop=','}},\n",
    "    \"items\": [{{#geneach 'items' num_iterations=5 join=', '}}\"{{gen 'this' temperature=0.7}}\"{{/geneach}}]\n",
    "}```\"\"\")\n",
    "\n",
    "# generate a character\n",
    "char = character_maker(\n",
    "    id=\"e1f491f7-7ab8-4dac-8c20-c92b5e7d883d\",\n",
    "    description=\"A quick and nimble fighter.\",\n",
    "    valid_weapons=valid_weapons, llm=llama\n",
    ")\n",
    "\n",
    "print(char)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:57.522736200Z",
     "start_time": "2023-05-24T20:46:57.509223300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define a recursive function that takes a jsonformer schema and returns a guidance program\n",
    "def jsonformer2guidance(schema, key=None, indent=0):\n",
    "    out = \"\"\n",
    "    if schema['type'] == 'object':\n",
    "        out += \"  \"*indent + \"{\\n\"\n",
    "        for k,v in schema['properties'].items():\n",
    "            out += \"  \"*(indent+1) + k + \": \" + jsonformer2guidance(v, k, indent+1) + \",\\n\"\n",
    "        out += \"  \"*indent + \"}\"\n",
    "        return out\n",
    "    elif schema['type'] == 'array':\n",
    "        if 'max_items' in schema:\n",
    "            extra_args = f\" max_iterations={schema['max_items']}\"\n",
    "        else:\n",
    "            extra_args = \"\"\n",
    "        return \"[{{#geneach '\"+key+\"' stop=']'\"+extra_args+\"}}{{#unless @first}}, {{/unless}}\" + jsonformer2guidance(schema['items'], \"this\") + \"{{/geneach}}]\"\n",
    "    elif schema['type'] == 'string':\n",
    "        return \"\\\"{{gen '\"+key+\"' stop='\\\"'}}\\\"\"\n",
    "    elif schema['type'] == 'number':\n",
    "        return \"{{gen '\"+key+\"' pattern='[0-9\\\\.]' stop=','}}\"\n",
    "    elif schema['type'] == 'boolean':\n",
    "        return \"{{#select '\"+key+\"'}}True{{or}}False{{/select}}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define a jsonformer schema\n",
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "        \"is_student\": {\"type\": \"boolean\"},\n",
    "        \"courses\": {\n",
    "            \"type\": \"array\",\n",
    "            \"max_items\": 6,\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run the guidance program\n",
    "prompt = \"Generate a person's information based on the following schema:\"\n",
    "program = guidance(prompt + \"\\n\" + jsonformer2guidance(json_schema))\n",
    "out = program(llm=llama)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "settings =  guidance.llms.LlamaCppSettings()\n",
    "settings.model = \"../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\"\n",
    "settings.n_gpu_layers = 10\n",
    "settings.n_threads = 12\n",
    "settings.tokenizer_name = \"openaccess-ai-collective/manticore-13b-chat-pyg\"\n",
    "settings.before_role = \"<|\"\n",
    "settings.after_role = \"|>\"\n",
    "llama = guidance.llms.LlamaCpp(settings=settings, caching=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# llama = guidance.llms.LlamaCpp(settings=settings, caching=False)\n",
    "experts = guidance('''\n",
    "{{#system~}}\n",
    "You are a helpful and terse assistant.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "I want a response to the following question:\n",
    "{{query}}\n",
    "Name 3 world-class experts (past or present) who would be great at answering this?\n",
    "Don't answer the question yet.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'expert_names' temperature=0 max_tokens=300}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0 max_tokens=500}}\n",
    "{{~/assistant}}\n",
    "''', llm=llama)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(experts(query='How can I be more productive?', llm=llama))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
