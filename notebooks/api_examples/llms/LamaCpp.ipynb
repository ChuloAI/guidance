{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# `LlamaCpp` examples\n",
    "\n",
    "This notebook contains usage examples of the llama-cpp-python support in guidance\n",
    "The first example is RPG character json generator.\n",
    "The second example takes a jsonformer scheme as input and generate a valid json.\n",
    "The third example uses the chat mode of guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:43.388084400Z",
     "start_time": "2023-05-24T20:46:40.833523300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /home/sclundbe/anaconda3/lib/python3.9/site-packages (0.1.53)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.1.55.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /home/sclundbe/anaconda3/lib/python3.9/site-packages (from llama-cpp-python) (4.5.0)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.55-cp39-cp39-linux_x86_64.whl size=209750 sha256=f959d826bb77102253f1f2e3346292ce59795128e46ef641c19a5da9fced1758\n",
      "  Stored in directory: /home/sclundbe/.cache/pip/wheels/0a/67/20/93fd94e6546d570666e2f1be50fc79e88678a6d8d95606b85d\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: llama-cpp-python\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama-cpp-python 0.1.53\n",
      "    Uninstalling llama-cpp-python-0.1.53:\n",
      "      Successfully uninstalled llama-cpp-python-0.1.53\n",
      "Successfully installed llama-cpp-python-0.1.55\n"
     ]
    }
   ],
   "source": [
    "# Install llama-cpp-python.\n",
    "!pip install llama-cpp-python -U # need version greater than 0.1.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import guidance.\n",
    "import guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:49.811939Z",
     "start_time": "2023-05-24T20:46:49.804877100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a LlamaCppSettings instance and set the model path and tokenizer name.\n",
    "settings = guidance.llms.LlamaCppSettings()\n",
    "settings.model = \"../../../../../../models/gpt4-x-vicuna-13B-GGML/gpt4-x-vicuna-13B.ggmlv3.q5_0.bin\" #../ggml-v2-models/WizardLM-7B-uncensored.ggml.q5_1.bin\"\n",
    "# Hugginface original tokenizer needed at the moment.\n",
    "settings.tokenizer_name = \"ehartford/WizardLM-7B-Uncensored\"\n",
    "\n",
    "# Other models would look like this:\n",
    "# settings.model = \"../ggml-v3-models/gpt4-x-vicuna-13B.ggmlv3.q5_1.bin\"\n",
    "# settings.tokenizer_name = \"TheBloke/gpt4-x-vicuna-13B-HF\"\n",
    "\n",
    "# Or like this:\n",
    "# settings.model = \"../ggml-v3-models/Manticore-13B.ggmlv3.q5_1.bin\"\n",
    "# settings.tokenizer_name = \"openaccess-ai-collective/manticore-13b\"\n",
    "\n",
    "# Set additional arguments like number of threads and gpu layers.\n",
    "settings.n_gpu_layers = 10\n",
    "settings.n_threads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:56.433152200Z",
     "start_time": "2023-05-24T20:46:55.570187500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../../../../../models/gpt4-x-vicuna-13B-GGML/gpt4-x-vicuna-13B.ggmlv3.q5_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 8 (mostly Q5_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size = 8535.27 MB\n",
      "llama_model_load_internal: mem required  = 10583.27 MB (+ 1608.00 MB per state)\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Create a LlamaCpp instance and pass the settings to it.\n",
    "llama = guidance.llms.LlamaCpp(\n",
    "    model = \"../../../../../models/gpt4-x-vicuna-13B-GGML/gpt4-x-vicuna-13B.ggmlv3.q5_0.bin\",\n",
    "    tokenizer=\"ehartford/WizardLM-7B-Uncensored\",\n",
    "    n_gpu_layers=10,\n",
    "    n_threads=12\n",
    ")\n",
    "\n",
    "# Other models would look like this:\n",
    "# model = \"../ggml-v3-models/gpt4-x-vicuna-13B.ggmlv3.q5_1.bin\"\n",
    "# tokenizer = \"TheBloke/gpt4-x-vicuna-13B-HF\"\n",
    "\n",
    "# Or like this:\n",
    "# model = \"../ggml-v3-models/Manticore-13B.ggmlv3.q5_1.bin\"\n",
    "# tokenizer = \"openaccess-ai-collective/manticore-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:57.522736200Z",
     "start_time": "2023-05-24T20:46:57.509223300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a character profile for an RPG game in JSON format.\n",
      "```json\n",
      "{\n",
      "    \"id\": \"e1f491f7-7ab8-4dac-8c20-c92b5e7d883d\",\n",
      "    \"description\": \"A quick and nimble fighter.\",\n",
      "    \"name\": \"Rayden\",\n",
      "    \"age\": 26,\n",
      "    \"armor\": \"leather\",\n",
      "    \"weapon\": \"sword\",\n",
      "    \"class\": \"fighter\",\n",
      "    \"mantra\": \"Swift and deadly, like a bolt of lightning!\",\n",
      "    \"strength\": 16,\n",
      "    \"items\": [\"health_potion\", \"mana_potion\", \"magic_wand\", \"sword\", \"leather_armor\"]\n",
      "}```\n"
     ]
    }
   ],
   "source": [
    "# 1st Example\n",
    "# we can pre-define valid option sets\n",
    "valid_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\n",
    "\n",
    "# define the prompt\n",
    "character_maker = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\n",
    "```json\n",
    "{\n",
    "    \"id\": \"{{id}}\",\n",
    "    \"description\": \"{{description}}\",\n",
    "    \"name\": \"{{gen 'name' temperature=0.7}}\",\n",
    "    \"age\": {{gen 'age' pattern='[0-9]+' max_tokens=2 stop=','}},\n",
    "    \"armor\": \"{{#select 'armor'}}leather{{or}}chainmail{{or}}plate{{/select}}\",\n",
    "    \"weapon\": \"{{select 'weapon' options=valid_weapons}}\",\n",
    "    \"class\": \"{{gen 'class'}}\",\n",
    "    \"mantra\": \"{{gen 'mantra' temperature=0.7}}\",\n",
    "    \"strength\": {{gen 'strength' pattern='[0-9]+' max_tokens=2 stop=','}},\n",
    "    \"items\": [{{#geneach 'items' num_iterations=5 join=', '}}\"{{gen 'this' temperature=0.7}}\"{{/geneach}}]\n",
    "}```\"\"\")\n",
    "\n",
    "# generate a character\n",
    "char = character_maker(\n",
    "    id=\"e1f491f7-7ab8-4dac-8c20-c92b5e7d883d\",\n",
    "    description=\"A quick and nimble fighter.\",\n",
    "    valid_weapons=valid_weapons, llm=llama, silent=True, caching=False\n",
    ")\n",
    "\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2nd Example\n",
    "# define a recursive function that takes a jsonformer schema and returns a guidance program\n",
    "def jsonformer2guidance(schema, key=None, indent=0):\n",
    "    out = \"\"\n",
    "    if schema['type'] == 'object':\n",
    "        out += \"  \"*indent + \"{\\n\"\n",
    "        for k,v in schema['properties'].items():\n",
    "            out += \"  \"*(indent+1) + k + \": \" + jsonformer2guidance(v, k, indent+1) + \",\\n\"\n",
    "        out += \"  \"*indent + \"}\"\n",
    "        return out\n",
    "    elif schema['type'] == 'array':\n",
    "        if 'max_items' in schema:\n",
    "            extra_args = f\" max_iterations={schema['max_items']}\"\n",
    "        else:\n",
    "            extra_args = \"\"\n",
    "        return \"[{{#geneach '\"+key+\"' stop=']'\"+extra_args+\"}}{{#unless @first}}, {{/unless}}\" + jsonformer2guidance(schema['items'], \"this\") + \"{{/geneach}}]\"\n",
    "    elif schema['type'] == 'string':\n",
    "        return \"\\\"{{gen '\"+key+\"' stop='\\\"'}}\\\"\"\n",
    "    elif schema['type'] == 'number':\n",
    "        return \"{{gen '\"+key+\"' pattern='[0-9\\\\.]' stop=','}}\"\n",
    "    elif schema['type'] == 'boolean':\n",
    "        return \"{{#select '\"+key+\"'}}True{{or}}False{{/select}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a jsonformer schema\n",
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "        \"is_student\": {\"type\": \"boolean\"},\n",
    "        \"courses\": {\n",
    "            \"type\": \"array\",\n",
    "            \"max_items\": 6,\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a person's information based on the following schema:\n",
      "{\n",
      "  name: \"John Doe\",\n",
      "  age: 3,\n",
      "  is_student: True,\n",
      "  courses: [\"Math\", \"Science\", \"English\"],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# run the guidance program\n",
    "prompt = \"Generate a person's information based on the following schema:\"\n",
    "program = guidance(prompt + \"\\n\" + jsonformer2guidance(json_schema))\n",
    "out = program(llm=llama, silent=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'tokenizer_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 3rd Example\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# settings =  guidance.llms.LlamaCppSettings()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# settings.model = \"../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# settings.before_role = \"<|\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# settings.after_role = \"|>\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m llama \u001b[39m=\u001b[39m guidance\u001b[39m.\u001b[39;49mllms\u001b[39m.\u001b[39;49mLlamaCpp(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     model \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     n_gpu_layers \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     n_threads \u001b[39m=\u001b[39;49m \u001b[39m12\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     tokenizer_name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mopenaccess-ai-collective/manticore-13b-chat-pyg\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     before_role \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m<|\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     after_role \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m|>\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     caching\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, silent\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'tokenizer_name'"
     ]
    }
   ],
   "source": [
    "# 3rd Example\n",
    "# settings =  guidance.llms.LlamaCppSettings()\n",
    "# settings.model = \"../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\"\n",
    "# settings.n_gpu_layers = 10\n",
    "# settings.n_threads = 12\n",
    "# settings.tokenizer_name = \"openaccess-ai-collective/manticore-13b-chat-pyg\"\n",
    "# settings.before_role = \"<|\"\n",
    "# settings.after_role = \"|>\"\n",
    "llama = guidance.llms.LlamaCpp(\n",
    "    model = \"../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\",\n",
    "    n_gpu_layers = 10,\n",
    "    n_threads = 12,\n",
    "    tokenizer = \"openaccess-ai-collective/manticore-13b-chat-pyg\",\n",
    "    before_role = \"<|\",\n",
    "    after_role = \"|>\",\n",
    "    caching=False, silent=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# llama = guidance.llms.LlamaCpp(settings=settings, caching=False)\n",
    "experts = guidance('''\n",
    "{{#system~}}\n",
    "You are a helpful and terse assistant.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "I want a response to the following question:\n",
    "{{query}}\n",
    "Name 3 world-class experts (past or present) who would be great at answering this?\n",
    "Don't answer the question yet.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'expert_names' temperature=0 max_tokens=300}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0 max_tokens=500}}\n",
    "{{~/assistant}}\n",
    "''', llm=llama)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(experts(query='How can I be more productive?', llm=llama))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
